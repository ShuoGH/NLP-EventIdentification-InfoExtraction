{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The labels that can be added \n",
    "\n",
    "As what I said in the notebook of '1. Identify the Breaking News Event', the pipeline of solving the task could be do **classification** then **clustering**.\n",
    "\n",
    "To do the classification, I need the training data set which have the labels indicating whether the news articles are about major breaking news or not. The classifier need such training data to get the knowledge.\n",
    "\n",
    "Another thing is that the current performance is still not that good, on the clustering work. The current solution is to use the word2vec trained on title to do clustering. There are some noise words in the corpus, influencing the performance. If could, some key words could be added into the data set (if possible). Key words could play a more important role in the representing the sentence/article. (although I know the keys words may coule be extracted using NLP tools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Some other exploreing experiences during this challenge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec implemented by Gensim\n",
    "\n",
    "`doc2vec` can be used to extract the sentence representations.\n",
    "\n",
    "I tried to use doc2vec here. Since there is no good pre-trained `doc2vec` model that I can use. The training corpus could influence the performance of model a lot. I didn't spend time to train the `doc2vec`. While `spacy` supplies good pre-trained model and API, I used that in my answer of this challenge.\n",
    "\n",
    "The args in doc2vec model:\n",
    "- vector_size: the length of the vectors got at last\n",
    "- window: the maximum distance between the current word to the predicted words\n",
    "- alpha: the learning rate \n",
    "- min_alpha: Learning rate will linearly drop to min_alpha as training progresses.\n",
    "- min_count: Ignore the words that total count lower than this.\n",
    "\n",
    "see the doc for more detail:https://radimrehurek.com/gensim/models/doc2vec.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2vec\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words_Tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lowestoft sea wall fall cyclist rescued friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>met gala game thrones feature week top comment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boy dies prom day allergic reaction pollen cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paddy jackson return rugby yet proof misogyny ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bafta awards stars prepare glitzy ceremony kil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Words_Tokenized\n",
       "0     lowestoft sea wall fall cyclist rescued friend\n",
       "1  met gala game thrones feature week top comment...\n",
       "2  boy dies prom day allergic reaction pollen cel...\n",
       "3  paddy jackson return rugby yet proof misogyny ...\n",
       "4  bafta awards stars prepare glitzy ceremony kil..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCorpus=pd.read_csv(\"data/news_corpus_for_doc2vec.csv\") \n",
    "modelCorpus.head()\n",
    "# The csv file was generated from \"1. Identify the Breaking News Event\" notebook.\n",
    "# The current csv file is generated from shorten data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0.. Doing training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa2y18/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10.. Doing training\n",
      "iteration 20.. Doing training\n",
      "iteration 30.. Doing training\n",
      "iteration 40.. Doing training\n",
      "iteration 50.. Doing training\n",
      "iteration 60.. Doing training\n",
      "iteration 70.. Doing training\n",
      "iteration 80.. Doing training\n",
      "iteration 90.. Doing training\n",
      "training finished.\n"
     ]
    }
   ],
   "source": [
    "# Only need to uncomment this when I need to re-train this model.\n",
    "max_epochs = 100\n",
    "alpha = 0.025\n",
    "\n",
    "documents = [TaggedDocument(content['Words_Tokenized'], [index]) for index, content in modelCorpus.iterrows()]\n",
    "model = Doc2Vec(documents, vector_size=100,alpha=alpha, min_alpha=0.00025, window=3, min_count=5)\n",
    "# model.build_vocab(documents)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    if not epoch%10:\n",
    "        print('iteration {0}.. Doing training'.format(epoch))\n",
    "    model.train(documents,total_examples=model.corpus_count,epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "print(\"training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save or load the model\n",
    "# model.save(\"doc2vecModel_corpus\")\n",
    "# model = Doc2Vec.load(\"doc2vecModel_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.docvecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT implemented by `pytorch_transformer`\n",
    "\n",
    "During the challenge, I tried to use BERT on my solution. The innotion of BERT is applying the bidirectional training of Transformer (Build all based on Attention Mechanism).\n",
    "\n",
    "`pytorch-transformer` supplies the BERT of `pytorch` version, and I can use the pre-trained model on my task. It's transfer learning and a head needs to be added for fine-tuninng. In the code repo, Hugging face building the net based on BERT for multiple NLP work, like the sentence classification, next sentence prediction, GLUE and NER task.  \n",
    "-`pytorch-transformer` https://github.com/huggingface/pytorch-transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "During task 1 of this challenge, I was trying to apply `BertForNextSentencePrediction` to tackle the problem. \n",
    "\n",
    "The idea is that to build corpus based on the news data set. See the title and description as the sentence pair of a specific news article. For a news article, the title and its description should have high similarity. `BertForNextSentencePrediction` can capture this relationship. I load the pre-trained model and only fine-tuned the head for my work.\n",
    "\n",
    "After training the model, use the model to measure the similarity of each title (can be seen as the Distance). From this, I could be able to do the clustering.\n",
    "\n",
    "Since I just learned the `BERT` and `Spacy` in these two days, I'm still on the stage of exercising demo and reading source code. Below code is the part of using `BERT` for the next sentence prediction.\n",
    "\n",
    "See the code details of this on [Using BERT NextSentence Prediction for Task1](Using BERT NextSentence Prediction for Task1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary \n",
    "\n",
    "To summary for this whole project, I did some NLP tasks.\n",
    "\n",
    "In task 1, I did some work about text cleaning, word vectors training and using DBSCAN to do the clustering. It works. though the performance is not very good.\n",
    "\n",
    "In task 2, NER is the major thing I did. The final results is kind of terrible. I should do more training on the models if want to get better results.\n",
    "\n",
    "I wanted to build some scripts to do the work. After finishing a small demo of the data set, I found the jupyter is a better choice for displaying. So the works are displayed by notebook.\n",
    "\n",
    "And in the last, thanks for echobox, for supplying me a such good challenge to finish. It makes me learn a lot during tackling the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
