{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use BERT next sentence prediction on task1 \n",
    "\n",
    "to measure the similarity of sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from pytorch_transformers import BertForNextSentencePrediction\n",
    "from pytorch_transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some explaination about the `transformers`. \n",
    "- don't need to add additional words into the vocab: https://github.com/google-research/bert/issues/396 \n",
    "- The tokenizer within the BERT can recognize the combined words. When working, it will seperate this kind of words into a word and a token beginning with `##`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == torch.device(\"cuda\"):\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(\"bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data \n",
    "\n",
    "Read from the csv file and only use the `ArticleTitle` and `ArticleDescription`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df=pd.read_excel(\"data/data_news_shorten.xlsx\")[['ArticleTitle','ArticleDescription']]\n",
    "df=df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleTitle</th>\n",
       "      <th>ArticleDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Turkey's finance minister says inflation will ...</td>\n",
       "      <td>https://m.haberturk.com/bakan-albayrak-tan-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Rangers boss Steven Gerrard has high hopes for...</td>\n",
       "      <td>STEVEN GERRARD hopes new signings Jordan Jones...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Explained: How Arsenal can pip Tottenham to to...</td>\n",
       "      <td>Arsenal’s hopes of finishing in the top four a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Canada-U.S. border transfers raise fear of del...</td>\n",
       "      <td>U.S. Customs and Border Protection says 731 no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ASVEL Villeurbanne wins French Cup, first sinc...</td>\n",
       "      <td>LDLC ASVEL Villeurbanne prevailed in the Frenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Urgent warning over MDMA after 15-year-old gir...</td>\n",
       "      <td>A 15-year-old girl has died after taking ecsta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Richie Ramsay and Robert MacIntyre in the hunt...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Rangers vs Celtic live stream: How to watch fi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Man's body found in wheelie bin behind Waitros...</td>\n",
       "      <td>Death 'being treated as unexplained', says Sco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Rich List: Jeremy Corbyn demands UK's 1,000 ri...</td>\n",
       "      <td>Corbyn says the Sunday Times Rich List shows \"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         ArticleTitle  \\\n",
       "10  Turkey's finance minister says inflation will ...   \n",
       "11  Rangers boss Steven Gerrard has high hopes for...   \n",
       "12  Explained: How Arsenal can pip Tottenham to to...   \n",
       "13  Canada-U.S. border transfers raise fear of del...   \n",
       "14  ASVEL Villeurbanne wins French Cup, first sinc...   \n",
       "15  Urgent warning over MDMA after 15-year-old gir...   \n",
       "16  Richie Ramsay and Robert MacIntyre in the hunt...   \n",
       "17  Rangers vs Celtic live stream: How to watch fi...   \n",
       "18  Man's body found in wheelie bin behind Waitros...   \n",
       "19  Rich List: Jeremy Corbyn demands UK's 1,000 ri...   \n",
       "\n",
       "                                   ArticleDescription  \n",
       "10    https://m.haberturk.com/bakan-albayrak-tan-o...  \n",
       "11  STEVEN GERRARD hopes new signings Jordan Jones...  \n",
       "12  Arsenal’s hopes of finishing in the top four a...  \n",
       "13  U.S. Customs and Border Protection says 731 no...  \n",
       "14  LDLC ASVEL Villeurbanne prevailed in the Frenc...  \n",
       "15  A 15-year-old girl has died after taking ecsta...  \n",
       "16                                                     \n",
       "17                                                     \n",
       "18  Death 'being treated as unexplained', says Sco...  \n",
       "19  Corbyn says the Sunday Times Rich List shows \"...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the data set\n",
    "\n",
    "The input features contains:\n",
    "- input id\n",
    "- mask\n",
    "- segment id\n",
    "\n",
    "\n",
    "To construct the pair, long sentence would be truncated so that the length of two sentences would not go beyond the max_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    '''\n",
    "    A single set of features of data.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, target):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.target = target\n",
    "\n",
    "def truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    '''\n",
    "    Truncates a sequence pair in place to the maximum length.\n",
    "    \n",
    "    It is just a heuristic which always truncate the longer sentence on token at a time.\n",
    "    Since we thought the each token in the shorter sentence contains more information.\n",
    "    '''\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence_pair(titles, descs, max_seq_length, tokenizer):\n",
    "    '''\n",
    "    Converter which can build the sentences pair (combining title & description)\n",
    "    args:\n",
    "        - titles: news title\n",
    "        - descs: the description of the news\n",
    "        - max_seq_length: the maximum length of the sentence\n",
    "        - tokenizer: tokenizer used here. \n",
    "    returns:\n",
    "        - features: list of defined class `InputFeatures`.\n",
    "    '''\n",
    "    features = []\n",
    "    for (ex_index, (title, desc)) in enumerate(zip(titles, descs)):\n",
    "        tokens_a = tokenizer.tokenize(title)\n",
    "\n",
    "        tokens_b = None\n",
    "        tokens_b = tokenizer.tokenize(desc)\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        \n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        \n",
    "        # If the tokens_b exists, then add the `sep` token so that it can be fed into network\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "            \n",
    "        # converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. \n",
    "        # Only real tokens are used。\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(\n",
    "                    input_ids=input_ids,\n",
    "                    input_mask=input_mask,\n",
    "                    segment_ids=segment_ids,\n",
    "                    target=1\n",
    "        ))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2019 09:03:07 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/shuo/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# cache_dir parameter can be added into the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tokenize(\"BAFTA TV Awards 2019: Stars prepare for glitzy ceremony as Killing Eve leads nominations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2019 09:03:09 - INFO - bert -   *** Example ***\n",
      "08/02/2019 09:03:09 - INFO - bert -   tokens: [CLS] lowest ##oft sea wall fall cyclist rescued by friend [SEP] coast ##guard ##s praise the boy ' s friend for his actions in rescuing him after he fell 15 ##ft in lowest ##oft . [SEP]\n",
      "08/02/2019 09:03:09 - INFO - bert -   input_ids: 101 7290 15794 2712 2813 2991 14199 10148 2011 2767 102 3023 18405 2015 8489 1996 2879 1005 1055 2767 2005 2010 4506 1999 23659 2032 2044 2002 3062 2321 6199 1999 7290 15794 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/02/2019 09:03:09 - INFO - bert -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/02/2019 09:03:09 - INFO - bert -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/02/2019 09:03:09 - INFO - bert -   *** Example ***\n",
      "08/02/2019 09:03:09 - INFO - bert -   tokens: [CLS] the met gala & ‘ game of throne ##s ’ feature in this week ’ s top comments round ##up [SEP] once again , our ever - sarcastic readers ##hip have let their opinions be known through a variety of biting , silly , and straight - up savage remarks . it is our happy duty to round up a selection of the very best , so that those of you not directly part ##aking in the ban ##ter may still enjoy all the light - hearted jokes made at [ … ] [SEP]\n",
      "08/02/2019 09:03:09 - INFO - bert -   input_ids: 101 1996 2777 16122 1004 1520 2208 1997 6106 2015 1521 3444 1999 2023 2733 1521 1055 2327 7928 2461 6279 102 2320 2153 1010 2256 2412 1011 22473 8141 5605 2031 2292 2037 10740 2022 2124 2083 1037 3528 1997 12344 1010 10021 1010 1998 3442 1011 2039 9576 12629 1012 2009 2003 2256 3407 4611 2000 2461 2039 1037 4989 1997 1996 2200 2190 1010 2061 2008 2216 1997 2017 2025 3495 2112 15495 1999 1996 7221 3334 2089 2145 5959 2035 1996 2422 1011 18627 13198 2081 2012 1031 1529 1033 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/02/2019 09:03:09 - INFO - bert -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/02/2019 09:03:09 - INFO - bert -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/02/2019 09:03:09 - INFO - bert -   *** Example ***\n",
      "08/02/2019 09:03:09 - INFO - bert -   tokens: [CLS] boy dies on prom day after allergic reaction to pollen while celebrating g ##cs ##es [SEP] joe dale ' s family have spoken out about losing their \" fit and active \" son to \" killer \" as ##mt ##ha to save other families from the everyday \" heart ##ache \" [SEP]\n",
      "08/02/2019 09:03:09 - INFO - bert -   input_ids: 101 2879 8289 2006 20877 2154 2044 27395 4668 2000 22482 2096 12964 1043 6169 2229 102 3533 8512 1005 1055 2155 2031 5287 2041 2055 3974 2037 1000 4906 1998 3161 1000 2365 2000 1000 6359 1000 2004 20492 3270 2000 3828 2060 2945 2013 1996 10126 1000 2540 15395 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/02/2019 09:03:09 - INFO - bert -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/02/2019 09:03:09 - INFO - bert -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/02/2019 09:03:09 - INFO - bert -   *** Example ***\n",
      "08/02/2019 09:03:09 - INFO - bert -   tokens: [CLS] paddy jackson ’ s return to rugby is yet more proof that mis ##ogy ##ny goes un ##pu ##nished [SEP] he may have been found not guilty of rape last year , but his sex ##ist behaviour is worth challenging . attitudes towards women aren ' t changing quickly enough , so perhaps laws should [SEP]\n",
      "08/02/2019 09:03:09 - INFO - bert -   input_ids: 101 16063 4027 1521 1055 2709 2000 4043 2003 2664 2062 6947 2008 28616 15707 4890 3632 4895 14289 28357 102 2002 2089 2031 2042 2179 2025 5905 1997 9040 2197 2095 1010 2021 2010 3348 2923 9164 2003 4276 10368 1012 13818 2875 2308 4995 1005 1056 5278 2855 2438 1010 2061 3383 4277 2323 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/02/2019 09:03:09 - INFO - bert -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/02/2019 09:03:09 - INFO - bert -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/02/2019 09:03:09 - INFO - bert -   *** Example ***\n",
      "08/02/2019 09:03:09 - INFO - bert -   tokens: [CLS] bafta tv awards 2019 : stars prepare for g ##litz ##y ceremony as killing eve leads nominations [SEP] stars are preparing for sunday night s tv bafta ##s with killing eve leading nominations at the g ##litz ##y ceremony actors will flock to the royal festival hall for the virgin media british academy television awards with bodyguard also set to rake in gong ##s phoebe waller - bridge ' s dark comic crime drama leads the nominations with its two main stars - jo ##die come ##r and sandra oh - in the running for leading . . . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2019 09:03:09 - INFO - bert -   input_ids: 101 22284 2694 2982 10476 1024 3340 7374 2005 1043 24725 2100 5103 2004 4288 6574 5260 9930 102 3340 2024 8225 2005 4465 2305 1055 2694 22284 2015 2007 4288 6574 2877 9930 2012 1996 1043 24725 2100 5103 5889 2097 19311 2000 1996 2548 2782 2534 2005 1996 6261 2865 2329 2914 2547 2982 2007 16174 2036 2275 2000 26008 1999 17242 2015 18188 23550 1011 2958 1005 1055 2601 5021 4126 3689 5260 1996 9930 2007 2049 2048 2364 3340 1011 8183 10265 2272 2099 1998 12834 2821 1011 1999 1996 2770 2005 2877 1012 1012 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/02/2019 09:03:09 - INFO - bert -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "08/02/2019 09:03:09 - INFO - bert -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "# Build the sentences pairs based on the corpus\n",
    "# I used the first 20 items for primary experiment\n",
    "df_shorten=df[:20]\n",
    "correct_pairs = convert_sentence_pair(df_shorten['ArticleTitle'].tolist(), \n",
    "                                      df_shorten['ArticleDescription'].tolist(), max_seq_length=200, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "\n",
    "Use the pre-trained model and do not do the fine-tuning, to see the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_transformers import BertConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2019 09:03:19 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/shuo/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "08/02/2019 09:03:19 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "08/02/2019 09:03:19 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/shuo/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pre-trained model\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction(config)\n",
    "\n",
    "# model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in correct_pairs], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in correct_pairs], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in correct_pairs], dtype=torch.long)\n",
    "\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
    "eval_dataloader = DataLoader(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# model.train()\n",
    "\n",
    "res = []\n",
    "for ids, seg, masks in eval_data:\n",
    "    oout = nn.functional.softmax(model(ids.reshape(\n",
    "        1, -1), seg.reshape(1, -1), masks.reshape(1, -1))[0], dim=1).detach().numpy()\n",
    "    res.append(oout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.30782852, 0.6921715 ]], dtype=float32),\n",
       " array([[0.31590047, 0.68409956]], dtype=float32),\n",
       " array([[0.32670474, 0.67329526]], dtype=float32),\n",
       " array([[0.30522758, 0.6947724 ]], dtype=float32),\n",
       " array([[0.3187479, 0.6812521]], dtype=float32),\n",
       " array([[0.3223731 , 0.67762685]], dtype=float32),\n",
       " array([[0.3234897, 0.6765103]], dtype=float32),\n",
       " array([[0.31943876, 0.6805613 ]], dtype=float32),\n",
       " array([[0.30497646, 0.69502354]], dtype=float32),\n",
       " array([[0.30947736, 0.6905226 ]], dtype=float32),\n",
       " array([[0.32563278, 0.67436725]], dtype=float32),\n",
       " array([[0.3160002 , 0.68399984]], dtype=float32),\n",
       " array([[0.31082058, 0.6891794 ]], dtype=float32),\n",
       " array([[0.33268175, 0.6673182 ]], dtype=float32),\n",
       " array([[0.31610614, 0.68389386]], dtype=float32),\n",
       " array([[0.31328642, 0.6867136 ]], dtype=float32),\n",
       " array([[0.36489668, 0.6351033 ]], dtype=float32),\n",
       " array([[0.3618721, 0.638128 ]], dtype=float32),\n",
       " array([[0.30842495, 0.69157505]], dtype=float32),\n",
       " array([[0.3103882, 0.6896118]], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the model:\n",
    "- For one sentences pair, the result is a [1,2] tensor of logits. The `output[0,0]` is the score of Next sentence being `True` and the `output[0,1]` is the score that Next sentence being `False`. \n",
    "- In the above code, `softmax` function is applied to the output. So the final output can be seen as the probability of whether Next sentence being `true` or `false`.\n",
    "\n",
    "From the results, it shows that the pre-trained model without further training can't behave well on capturing the relation between sentence pair. The input sentence pairs are all pairs of **Title & Description**. The results of the first 20 news data set shows that the next sentence being False has higher probability.\n",
    "\n",
    "To make the model behave better on my corpus, the training is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "res_c=np.concatenate(res)\n",
    "# print(res_c)\n",
    "_ = plt.hist(res_c, bins=100,color=['b','r'])\n",
    "plt.show()\n",
    "# The figure show that the performance is really not good "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The locations of blue columns should show the probablity of Next sentence being True, while red ones show the Next sentence being False. The current model doesn't behave well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test single pair sentences to get familiar with it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2019 09:15:43 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/shuo/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "08/02/2019 09:15:43 - INFO - bert -   *** Example ***\n",
      "08/02/2019 09:15:43 - INFO - bert -   tokens: [CLS] dj ##oko ##vic pre ##va ##ils over ts ##its ##ip ##as to claim third madrid title [SEP] world no . 1 novak dj ##oko ##vic claimed a third madrid open crown by defeating stefano ##s ts ##its ##ip [SEP]\n",
      "08/02/2019 09:15:43 - INFO - bert -   input_ids: 101 6520 16366 7903 3653 3567 12146 2058 24529 12762 11514 3022 2000 4366 2353 6921 2516 102 2088 2053 1012 1015 19580 6520 16366 7903 3555 1037 2353 6921 2330 4410 2011 6324 19618 2015 24529 12762 11514 102\n",
      "08/02/2019 09:15:43 - INFO - bert -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "08/02/2019 09:15:43 - INFO - bert -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    }
   ],
   "source": [
    "# custom test for single \n",
    "# title=[\"Meanwhile: For a Knife, Dagger, Sword, Machete or Zombie-Killer, Just Ask These Ladies\"]\n",
    "title=['Djokovic prevails over Tsitsipas to claim third Madrid title']\n",
    "\n",
    "# descri=[\" Whitehead’s Cutlery in Butte, Mont., is 128 years old and will gladly sharpen scissors sold generations ago.\"]\n",
    "descri=['World No.1 Novak Djokovic claimed a third Madrid Open crown by defeating Stefanos Tsitsipas 6-3, 6-4.']\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentences_pairs=convert_sentence_pair(title,descri,40,tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_input_id=torch.tensor(sentences_pairs[0].input_ids).reshape(1,-1)\n",
    "single_input_mask=torch.tensor(sentences_pairs[0].input_mask).reshape(1,-1)\n",
    "single_segment_id=torch.tensor(sentences_pairs[0].segment_ids).reshape(1,-1)\n",
    "\n",
    "# single_input_ids = torch.tensor([f.input_ids for f in correct_pairs], dtype=torch.long)\n",
    "# all_input_mask = torch.tensor([f.input_mask for f in correct_pairs], dtype=torch.long)\n",
    "# all_segment_ids = torch.tensor([f.segment_ids for f in correct_pairs], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3165,  0.0569]], grad_fn=<AddmmBackward>),)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForNextSentencePrediction(config)\n",
    "model.eval()\n",
    "model(single_input_id,single_segment_id,single_input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limited time, I didn't explore BERT more for this task. Using `BertForNextSentencePrediction` could be a idea for this task, but the model needs to be trained so that it can know better on the our corpus.\n",
    "\n",
    "More could do:\n",
    "- Fine tune. Huggig face also supply the script to guide how to fine tune. Using the https://github.com/huggingface/pytorch-transformers/blob/master/examples/lm_finetuning/simple_lm_finetuning.py \n",
    "- I did the NER using `spacy` this time. BERT is also capable to do NER, further work could be exploring the variation of BERT on multiple task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "285px",
    "left": "1090px",
    "right": "20px",
    "top": "2px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
